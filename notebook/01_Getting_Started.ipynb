{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Chapter 1: Getting Started\n",
                "\n",
                "**Data-Juicer User Guide**\n",
                "\n",
                "- Git Commit: `v1.0.5`\n",
                "- Commit Date: 2026-01-16\n",
                "- Repository: https://github.com/datajuicer/data-juicer\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Table of Contents\n",
                "\n",
                "1. [Install Data-Juicer](#install-data-juicer)\n",
                "2. [Create Sample JSONL Data](#create-sample-jsonl-data)\n",
                "3. [Write Basic YAML Config](#write-basic-yaml-config)\n",
                "4. [Execute Pipeline](#execute-pipeline)\n",
                "5. [Check Output](#check-output)\n",
                "6. [Learning Path](#learning-path)\n",
                "   - [Core Concepts (Recommended Order)](#core-concepts-recommended-order)\n",
                "   - [Advanced Topics (Appendices)](#advanced-topics-appendices)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Install Data-Juicer\n",
                "\n",
                "Data-Juicer can be easily installed via pip. We recommend using `uv` for faster installation, but standard `pip` works too.\n",
                "\n",
                "Detailed installation tutorial [here](https://datajuicer.github.io/data-juicer/en/main/docs/tutorial/Installation.html)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!uv pip install py-data-juicer"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Create Sample JSONL Data\n",
                "\n",
                "Data-Juicer works with JSONL (JSON Lines) format, where each line is a valid JSON object. This format is efficient for streaming large datasets and is widely used in the ML community."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import os\n",
                "\n",
                "# Create data directory\n",
                "os.makedirs('./data', exist_ok=True)\n",
                "\n",
                "# Sample data\n",
                "samples = [\n",
                "    {\"text\": \"Today is Sunday and it's a happy day!\", \"meta\": {\"src\": \"web\", \"date\": \"2024-01-01\"}},\n",
                "    {\"text\": \"Do you need a cup of coffee?\", \"meta\": {\"src\": \"social\", \"author\": \"user123\"}},\n",
                "    {\"text\": \"Machine learning is transforming the world.\", \"meta\": {\"src\": \"article\"}},\n",
                "    {\"text\": \"Short.\", \"meta\": {\"src\": \"web\"}},\n",
                "    {\"text\": \"This is a longer text with more content to demonstrate filtering capabilities.\", \"meta\": {\"src\": \"blog\"}}\n",
                "]\n",
                "\n",
                "# Write JSONL file\n",
                "with open('./data/sample.jsonl', 'w') as f:\n",
                "    for sample in samples:\n",
                "        f.write(json.dumps(sample) + '\\n')\n",
                "\n",
                "print(f\"Created sample dataset with {len(samples)} samples\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Write Basic YAML Config\n",
                "\n",
                "Data-Juicer uses YAML configuration files (\"recipes\") to define processing pipelines. A recipe specifies:\n",
                "- **Input/Output paths**: Where to read and write data\n",
                "- **Processing operators**: What transformations to apply\n",
                "- **Execution settings**: Parallelism, caching, etc.\n",
                "\n",
                "Let's create a simple recipe that filters text by length and language, then removes duplicates."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "config = \"\"\"# Basic Data-Juicer Configuration\n",
                "project_name: 'getting_started'\n",
                "\n",
                "# Input/Output paths\n",
                "dataset_path: './data/sample.jsonl'\n",
                "export_path: './outputs/processed.jsonl'\n",
                "\n",
                "# Number of parallel processes\n",
                "np: 1\n",
                "\n",
                "# Processing pipeline\n",
                "process:\n",
                "  # 1. Filter by text length\n",
                "  - text_length_filter:\n",
                "      min_len: 10\n",
                "      max_len: 200\n",
                "  \n",
                "  # 2. Filter by language (English)\n",
                "  - language_id_score_filter:\n",
                "      lang: 'en'\n",
                "      min_score: 0.8\n",
                "  \n",
                "  # 3. Remove duplicates\n",
                "  - document_deduplicator:\n",
                "      lowercase: true\n",
                "\"\"\"\n",
                "\n",
                "# Save config\n",
                "os.makedirs('./configs', exist_ok=True)\n",
                "with open('./configs/basic.yaml', 'w') as f:\n",
                "    f.write(config)\n",
                "\n",
                "print(\"Config saved to ./configs/basic.yaml\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Execute Pipeline\n",
                "\n",
                "Data-Juicer provides two ways to run pipelines:\n",
                "1. **Command-line**: Using the `dj-process` command\n",
                "2. **Programmatic**: Using Python API for more control\n",
                "\n",
                "Both methods produce identical results. Choose based on your workflow preference."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!dj-process --config ./configs/basic.yaml"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "or running programmatically"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from data_juicer.config import init_configs\n",
                "cfg = init_configs(['--config', './configs/basic.yaml'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from data_juicer.core import DefaultExecutor\n",
                "\n",
                "# Create and run executor\n",
                "executor = DefaultExecutor(cfg)\n",
                "dataset = executor.run()\n",
                "\n",
                "print(\"\\nProcessed samples:\")\n",
                "for i, sample in enumerate(dataset):\n",
                "    print(f\"{i+1}. {sample['text']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Check Output\n",
                "\n",
                "Let's verify the results. Notice how the pipeline filtered out:\n",
                "- Short texts (< 10 characters)\n",
                "- Non-English texts (language score < 0.8)\n",
                "- Duplicate entries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "\n",
                "# Read processed data\n",
                "with open('./outputs/processed.jsonl', 'r') as f:\n",
                "    processed = [json.loads(line) for line in f]\n",
                "\n",
                "print(f\"Original samples: 5\")\n",
                "print(f\"Processed samples: {len(processed)}\")\n",
                "print(\"\\nProcessed data:\")\n",
                "for i, sample in enumerate(processed, 1):\n",
                "    print(f\"\\n{i}. {sample['text'][:60]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Learning Path\n",
                "\n",
                "Here's a recommended learning path through the remaining chapters:\n",
                "\n",
                "### Core Concepts (Recommended Order)\n",
                "\n",
                "1. **[Chapter 2: Building Recipes](./02_Building_Recipes.ipynb)**\n",
                "   - Understand recipe structure (global parameters, process pipeline, operator parameters)\n",
                "   - Create basic and custom recipes\n",
                "   - Override parameters via CLI\n",
                "   - Explore pre-defined recipes from the Recipe Gallery (data-juicer-hub)\n",
                "\n",
                "2. **[Chapter 3: Data Formats](./03_Data_Formats.ipynb)**\n",
                "   - Learn Data-Juicer's unified format (DJ Format)\n",
                "   - Convert between dialog formats (Messages, ShareGPT, Alpaca, Query-Response)\n",
                "   - Handle multimodal format conversion (LLaVA, MMC4, InternVid, etc.)\n",
                "\n",
                "3. **[Chapter 4: Operators Usage](./04_Operators_Usage.ipynb)**\n",
                "   - Use operators programmatically via Python API\n",
                "   - Chain operators sequentially or batch process\n",
                "   - Inspect operator statistics\n",
                "\n",
                "4. **[Chapter 5: Analysis & Visualization](./05_Analysis_and_Visualization.ipynb)**\n",
                "   - Run data analysis with `dj-analyze`\n",
                "   - Interpret statistics and visualizations\n",
                "   - Compare datasets before and after processing\n",
                "\n",
                "5. **[Chapter 6: Distributed Processing with Ray](./06_Distributed_Processing_with_Ray.ipynb)**\n",
                "   - Set up Ray clusters (local and multi-node)\n",
                "   - Use demo configs from `demos/process_on_ray/`\n",
                "   - Monitor resources via Ray Dashboard\n",
                "   - Run distributed deduplication\n",
                "\n",
                "### Advanced Topics\n",
                "\n",
                "- **[Chapter 7: Pre-processing](./07_Preprocessing.ipynb)**\n",
                "  - Split datasets by language\n",
                "  - Convert raw formats (arXiv, Stack Exchange) to JSONL\n",
                "  - Serialize complex metadata fields\n",
                "\n",
                "- **[Chapter 8: Multimodal Data Processing](./08_Multimodal_Data_Processing.ipynb)**\n",
                "  - Understand multimodal format with special tokens\n",
                "  - Process image-text, video-text, audio-text data\n",
                "  - Convert between multimodal formats (LLaVA, Video-ChatGPT, WavCaps, etc.)\n",
                "  - Apply multimodal operators (image/video/audio filters)\n",
                "\n",
                "- **[Chapter 9: Advanced Dataset Configuration](./09_Advanced_Dataset_Configuration.ipynb)**\n",
                "  - Mix multiple datasets with custom weights\n",
                "  - Sample subsets from large datasets\n",
                "  - Map custom field names to standard fields\n",
                "  - Load remote datasets from HuggingFace\n",
                "\n",
                "## Additional Resources\n",
                "\n",
                "- **Documentation**: https://datajuicer.github.io/data-juicer\n",
                "- **GitHub**: https://github.com/datajuicer/data-juicer\n",
                "- **Recipe Gallery**: https://datajuicer.github.io/data-juicer-hub\n",
                "- **Operator Reference**: https://datajuicer.github.io/data-juicer/en/main/docs/Operators.html"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "data-juicer-nk",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
