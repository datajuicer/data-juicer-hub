{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 3: Data Formats and Loading\n",
        "\n",
        "**Data-Juicer User Guide**\n",
        "\n",
        "- Git Commit: `v1.4.5`\n",
        "- Commit Date: 2026-01-16\n",
        "- Repository: https://github.com/datajuicer/data-juicer\n",
        "\n",
        "---\n",
        "\n",
        "This chapter answers the most common question: \"Can my data be used with Data-Juicer, and how do I load it?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table of Contents\n",
        "\n",
        "1. [Supported File Formats](#supported-file-formats)\n",
        "2. [DJ Format Specification](#dj-format-specification)\n",
        "3. [Field Mapping](#field-mapping)\n",
        "4. [Loading Data](#loading-data)\n",
        "5. [Format Compatibility Quick Reference](#format-compatibility-quick-reference)\n",
        "6. [Related Tools](#related-tools)\n",
        "7. [Further Reading](#further-reading)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "import json\n",
        "import os\n",
        "\n",
        "os.makedirs('./data', exist_ok=True)\n",
        "os.makedirs('./configs', exist_ok=True)\n",
        "\n",
        "samples = [\n",
        "    {\"title\": \"Introduction to ML\", \"text\": \"Machine learning is a subset of AI that enables systems to learn from data.\"},\n",
        "    {\"title\": \"Data-Juicer 2.0: Cloud-Scale Adaptive Data Processing for and with Foundation Models\", \"text\": \"error text\"},\n",
        "    {\"title\": \"Hi\", \"text\": \"This is a long text, which is longer than 20 chars.\"}\n",
        "]\n",
        "\n",
        "with open('./data/sample.jsonl', 'w') as f:\n",
        "    for s in samples:\n",
        "        f.write(json.dumps(s) + '\\n')\n",
        "\n",
        "print(f\"‚úÖ Created sample.jsonl with {len(samples)} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Supported File Formats\n",
        "\n",
        "Data-Juicer natively supports multiple file formats through its formatter system:\n",
        "\n",
        "| Formatter | Supported Extensions | Description |\n",
        "|-----------|---------------------|-------------|\n",
        "| **JsonFormatter** | `.json`, `.jsonl`, `.jsonl.zst` | JSON and JSON Lines (recommended) |\n",
        "| **ParquetFormatter** | `.parquet` | Apache Parquet (efficient for large datasets) |\n",
        "| **CsvFormatter** | `.csv` | Comma-separated values |\n",
        "| **TsvFormatter** | `.tsv` | Tab-separated values |\n",
        "| **TextFormatter** | `.txt`, `.md`, `.pdf`, `.docx`, code files | Plain text and documents |\n",
        "\n",
        "### TextFormatter: Extended Support\n",
        "\n",
        "- **Documents**: `.txt`, `.md`, `.pdf`, `.docx`, `.tex`, `.rst`\n",
        "- **Code files**: `.py`, `.java`, `.cpp`, `.js`, `.ts`, `.go`, `.rs`, `.rb`, `.php`, `.sql`, `.sh`, `.html`, `.css`, `.xml`, and more\n",
        "\n",
        "For PDF and DOCX files, Data-Juicer automatically extracts text content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using the YAML Way"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process using the default 'text' field\n",
        "config = \"\"\"project_name: 'basic_demo'\n",
        "dataset_path: './data/sample.jsonl'\n",
        "export_path: './outputs/basic_processed.jsonl'\n",
        "\n",
        "process:\n",
        "  - text_length_filter:\n",
        "      min_len: 15   # Filter out samples with text shorter than 15 chars\n",
        "\"\"\"\n",
        "\n",
        "with open('./configs/basic.yaml', 'w') as f:\n",
        "    f.write(config)\n",
        "\n",
        "!dj-process --config ./configs/basic.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check results\n",
        "print(\"Processed output (samples with text >= 15 chars):\")\n",
        "with open('./outputs/basic_processed.jsonl', 'r') as f:\n",
        "    for line in f:\n",
        "        print(json.loads(line))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Other formats can be loaded in the same way:\n",
        "\n",
        "```yaml\n",
        "# JSONL (most common)\n",
        "dataset_path: './data/train.jsonl'\n",
        "\n",
        "# Parquet (efficient for large datasets)\n",
        "dataset_path: './data/train.parquet'\n",
        "\n",
        "# CSV\n",
        "dataset_path: './data/train.csv'\n",
        "\n",
        "# Specify file suffixes explicitly\n",
        "dataset_path: './data/'\n",
        "suffixes: ['.csv', '.json']  # Only load these file types\n",
        "```\n",
        "\n",
        "For complex data loading scenarios (data mixing, sampling, remote datasets), see **[Chapter 10: Advanced Dataset Configuration](./10_Advanced_Dataset_Configuration.ipynb)**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## DJ Format Specification\n",
        "\n",
        "While Data-Juicer supports multiple file formats, the **data structure** follows a unified schema.\n",
        "\n",
        "### Core Fields\n",
        "\n",
        "```python\n",
        "{\n",
        "  \"text\": \"xxx\",           # For pretraining and general language modeling\n",
        "  \"query\": \"xxx\",          # For dialog and question-answering\n",
        "  \"response\": \"xxx\",       # For dialog responses and assistant output\n",
        "}\n",
        "```\n",
        "\n",
        "**Note**: Different dataset types use different core fields. A dialog dataset would have `query` and `response`, while a pretraining dataset would have only `text`.\n",
        "\n",
        "### Multimodal Fields\n",
        "\n",
        "For multimodal data, DJ Format uses **file paths** (not embedded data):\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"text\": \"<__dj__image> A beautiful sunset over the ocean. <|__dj__eoc|>\",\n",
        "  \"images\": [\"path/to/sunset.jpg\"],\n",
        "  \"audios\": [\"path/to/narration.wav\"],\n",
        "  \"videos\": [\"path/to/clip.mp4\"]\n",
        "}\n",
        "```\n",
        "\n",
        "**Special Tokens** (configurable in YAML):\n",
        "- `<__dj__image>`: Image placeholder (config: `image_special_token`)\n",
        "- `<__dj__audio>`: Audio placeholder (config: `audio_special_token`)\n",
        "- `<__dj__video>`: Video placeholder (config: `video_special_token`)\n",
        "- `<|__dj__eoc|>`: End of chunk (config: `eoc_special_token`)\n",
        "\n",
        "### Metadata Fields\n",
        "\n",
        "Optional fields for tracking data lineage:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"text\": \"Sample text...\",\n",
        "  \"meta\": {\"source\": \"wikipedia\", \"date\": \"2024-01\"},\n",
        "  \"stats\": {\"lang\": \"en\", \"text_length\": 256}\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Field Mapping\n",
        "\n",
        "You don't need to rename your fields. Data-Juicer supports field mapping to work with any field names.\n",
        "\n",
        "### Text Field Mapping (`text_keys`)\n",
        "\n",
        "If your data uses a different field name for text content:\n",
        "\n",
        "```yaml\n",
        "# Your data: {\"content\": \"Hello world\", \"id\": 1}\n",
        "text_keys: 'content'   # Map 'content' as the text field\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using 'title' Field Instead of 'text'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process using 'title' field instead of 'text'\n",
        "config = \"\"\"project_name: 'field_mapping_demo'\n",
        "dataset_path: './data/sample.jsonl'\n",
        "export_path: './outputs/title_processed.jsonl'\n",
        "\n",
        "# Field mapping: use 'title' as the text field\n",
        "text_keys: 'title'\n",
        "\n",
        "process:\n",
        "  - text_length_filter:\n",
        "      min_len: 15   # Filter out samples with title shorter than 15 chars\n",
        "\"\"\"\n",
        "\n",
        "with open('./configs/field_mapping.yaml', 'w') as f:\n",
        "    f.write(config)\n",
        "\n",
        "!dj-process --config ./configs/field_mapping.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check results\n",
        "print(\"Processed output (samples with title >= 15 chars):\")\n",
        "with open('./outputs/title_processed.jsonl', 'r') as f:\n",
        "    for line in f:\n",
        "        print(json.loads(line))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multiple Text Fields\n",
        "\n",
        "When your data has **multiple text fields** that different operators need to process, declare them all in the global config:\n",
        "\n",
        "```yaml\n",
        "# Declare ALL text fields upfront\n",
        "text_keys: ['title', 'text']\n",
        "```\n",
        "\n",
        "**Why declare multiple `text_keys` globally?**\n",
        "- During loading, Data-Juicer validates that all declared fields exist\n",
        "- Prevents errors when operators access undeclared fields\n",
        "- If an operator doesn't specify `text_key`, it uses the **first** one in the list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Different Operators for Different Fields"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process with different operators for different fields\n",
        "config = \"\"\"project_name: 'multi_field_demo'\n",
        "dataset_path: './data/sample.jsonl'\n",
        "export_path: './outputs/multi_field_processed.jsonl'\n",
        "\n",
        "# Declare ALL text fields upfront\n",
        "text_keys: ['title', 'text']\n",
        "\n",
        "process:\n",
        "  - text_length_filter:\n",
        "      text_key: 'title'    # Process 'title' field\n",
        "      min_len: 15\n",
        "  \n",
        "  - words_num_filter:      # Filter by total word count (>= 5)\n",
        "      text_key: 'text'     # Process 'text' field  \n",
        "      min_hum: 5\n",
        "\"\"\"\n",
        "\n",
        "with open('./configs/multi_field.yaml', 'w') as f:\n",
        "    f.write(config)\n",
        "\n",
        "!dj-process --config ./configs/multi_field.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check results\n",
        "with open('./outputs/multi_field_processed.jsonl', 'r') as f:\n",
        "    for line in f:\n",
        "        print(json.loads(line))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multimodal Field Mapping\n",
        "\n",
        "Similarly for images, audio, and video:\n",
        "\n",
        "```yaml\n",
        "# Default field names\n",
        "image_key: 'images'\n",
        "audio_key: 'audios'\n",
        "video_key: 'videos'\n",
        "\n",
        "# Custom mapping:\n",
        "image_key: 'img_paths'   # Your field is called 'img_paths'\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## More Data Loading Options"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading Multiple Files from a Directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a directory with multiple text files\n",
        "os.makedirs('./data/multiple_json', exist_ok=True)\n",
        "\n",
        "multiple_json = [\n",
        "    (\"intro.json\", \"Machine learning is a powerful technology that enables computers to learn from data.\"),\n",
        "    (\"deep_learning.json\", \"Deep learning is a subset of machine learning using neural networks.\"),\n",
        "    (\"short.json\", \"Hi.\")\n",
        "]\n",
        "\n",
        "for filename, content in multiple_json:\n",
        "    data = {\"text\": content}\n",
        "    with open(f'./data/multiple_json/{filename}', 'w', encoding='utf-8') as f:\n",
        "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"‚úÖ Created text files:\")\n",
        "for filename, content in multiple_json:\n",
        "    print(f\"  - {filename}: '{content[:40]}...'\" if len(content) > 40 else f\"  - {filename}: '{content}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load all .txt files from directory\n",
        "config = \"\"\"project_name: 'directory_demo'\n",
        "dataset_path: './data/multiple_json/'\n",
        "\n",
        "suffixes: ['.json']\n",
        "\n",
        "export_path: './outputs/json_processed.jsonl'\n",
        "\n",
        "process:\n",
        "  - text_length_filter:\n",
        "      min_len: 20\n",
        "\"\"\"\n",
        "\n",
        "with open('./configs/directory.yaml', 'w') as f:\n",
        "    f.write(config)\n",
        "\n",
        "!dj-process --config ./configs/directory.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check results - \"Hi.\" should be filtered out\n",
        "print(\"Processed output (text files with >= 20 chars):\")\n",
        "with open('./outputs/json_processed.jsonl', 'r') as f:\n",
        "    for line in f:\n",
        "        data = json.loads(line)\n",
        "        print(f\"  - {data['text'][:60]}...\" if len(data['text']) > 60 else f\"  - {data['text']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### HuggingFace Hub\n",
        "\n",
        "Load datasets directly from HuggingFace Hub:\n",
        "\n",
        "```yaml\n",
        "dataset:\n",
        "  configs:\n",
        "    - type: 'remote'\n",
        "      source: 'huggingface'\n",
        "      path: \"HuggingFaceFW/fineweb\"\n",
        "      name: \"CC-MAIN-2024-10\"\n",
        "      split: \"train\"\n",
        "      limit: 1000\n",
        "```\n",
        "\n",
        "\n",
        "### Cloud Storage (S3)\n",
        "\n",
        "Load data from S3-compatible storage:\n",
        "\n",
        "```yaml\n",
        "# Load all JSON files from an S3 directory\n",
        "dataset:\n",
        "  path: s3://my-bucket/data/json-files/\n",
        "  format: json  # Must specify format for directory paths\n",
        "  aws_access_key_id: xxx\n",
        "  aws_secret_access_key: xxx\n",
        "\n",
        "# Load all Parquet files from an S3 directory\n",
        "dataset:\n",
        "  path: s3://my-bucket/data/parquet-files/\n",
        "  format: parquet\n",
        "  aws_access_key_id: xxx\n",
        "  aws_secret_access_key: xxx\n",
        "```\n",
        "\n",
        "For more data loading configurations, please refer to [Dataset Configuration](https://datajuicer.github.io/data-juicer/en/main/docs/DatasetCfg.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Format Compatibility Quick Reference\n",
        "\n",
        "| Your Data Format | Can Use Directly? | Action Needed |\n",
        "|------------------|-------------------|---------------|\n",
        "| JSONL with `text` field | ‚úÖ Yes | None |\n",
        "| JSONL with other field names | ‚úÖ Yes | Set `text_keys: 'your_field'` |\n",
        "| Parquet / CSV / TSV | ‚úÖ Yes | Set `text_keys` if needed |\n",
        "| Plain text files (.txt, .md) | ‚úÖ Yes | Each file ‚Üí one sample |\n",
        "| PDF / DOCX | ‚úÖ Yes | Text auto-extracted |\n",
        "| ShareGPT format | ‚ùå Convert first | Use `xxx_sharegpt_to_dj.py` |\n",
        "| Alpaca format | ‚ùå Convert first | Use `alpaca_to_dj.py` |\n",
        "| Messages format (OpenAI-style) | ‚ùå Convert first | Use `messages_to_dj.py` |\n",
        "| LLaVA (image-text) | ‚ùå Convert first | Use `llava_to_dj.py` |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Related Tools\n",
        "\n",
        "Data-Juicer provides additional tools for data preparation:\n",
        "\n",
        "| Tool Category | Description | Documentation | Notebook |\n",
        "|---------------|-------------|---------------|-----------|\n",
        "| **Format Conversion** | Convert between formats (ShareGPT, Alpaca, LLaVA, etc.) | [üìñ post_tuning_dialog](https://datajuicer.github.io/data-juicer/en/main/tools/fmt_conversion/post_tuning_dialog/README.html) [üìñ Multimodal](https://datajuicer.github.io/data-juicer/en/main/tools/fmt_conversion/multimodal/README.html) | [Chapter 9: Multimodal Data Processing](./09_Multimodal_Data_Processing.ipynb) |\n",
        "| **Preprocessing** | Prepare raw data before processing | [üìñ preprocess](https://datajuicer.github.io/data-juicer/en/main/tools/preprocess/README.html) | [Chapter 8: Preprocessing](./08_Preprocessing.ipynb) |\n",
        "| **Postprocessing** | Transform processed data for downstream tasks | [üìñ postprocess](https://datajuicer.github.io/data-juicer/en/main/tools/postprocess/README.html) | \\ |\n",
        "\n",
        "These tools are located in `tools/` directory of the Data-Juicer repository."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Further Reading\n",
        "\n",
        "- üìñ [Dataset Configuration](https://datajuicer.github.io/data-juicer/en/main/docs/DatasetCfg.html)\n",
        "- üìñ [Configuration Reference](https://github.com/datajuicer/data-juicer/blob/main/data_juicer/config/config_all.yaml)\n",
        "- üìñ [Format Conversion Documentation](https://datajuicer.github.io/data-juicer/en/main/tools/fmt_conversion/README.html)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "data-juicer-nk",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
