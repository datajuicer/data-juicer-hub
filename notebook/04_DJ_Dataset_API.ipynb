{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 4: DJ Dataset API\n",
        "\n",
        "**Data-Juicer User Guide**\n",
        "\n",
        "- Git Commit: `v1.4.5`\n",
        "- Commit Date: 2026-01-16\n",
        "- Repository: https://github.com/datajuicer/data-juicer\n",
        "\n",
        "---\n",
        " \n",
        "> **Note:** This chapter is intended for users who want to programmatically call Data-Juicer or are familiar with HuggingFace Dataset operations.  \n",
        "> If you only care about YAML-based invocation, you can skip this chapter.\n",
        "\n",
        "Data-Juicer provides two dataset implementations:\n",
        "- **NestedDataset**: Built on HuggingFace Datasets, for single-machine processing\n",
        "- **RayDataset**: Built on Ray Data, for distributed processing\n",
        "\n",
        "Both share the same `DJDataset` interface, so you can switch backends without changing your operator code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table of Contents\n",
        "\n",
        "1. [Quick Comparison](#quick-comparison)\n",
        "2. [NestedDataset: HuggingFace-Compatible API](#nesteddataset-huggingface-compatible-api)\n",
        "3. [Data-Juicer Enhancements](#data-juicer-enhancements)\n",
        "4. [RayDataset (Distributed)](#raydataset-distributed)\n",
        "5. [Production Usage: Via Configuration](#production-usage-via-configuration)\n",
        "6. [Key Differences](#key-differences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Data-Juicer (if not installed)\n",
        "# !uv pip install py-data-juicer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick Comparison\n",
        "\n",
        "| Feature | Pandas | HuggingFace | Data-Juicer |\n",
        "|---------|--------|-------------|-------------|\n",
        "| **Base** | NumPy | Arrow | Built on HF |\n",
        "| **Indexing** | `df['col']` | `ds['col']` | Same + **nested access** (`ds['meta.source']`) |\n",
        "| **Processing** | `.apply()` | `.map()`, `.filter()` | Same + **100+ operators** via `.process()` |\n",
        "| **Multimodal** | Manual | Supported | **Lazy loading** for efficiency |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## NestedDataset: HuggingFace-Compatible API\n",
        "\n",
        "`NestedDataset` is fully compatible with HuggingFace Datasets API, so you can use familiar operations directly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# HuggingFace-style API works directly\n",
        "from data_juicer.core.data import NestedDataset\n",
        "\n",
        "# Create dataset (same as HuggingFace)\n",
        "ds = NestedDataset.from_dict({\n",
        "    'text': ['Hello world', 'Data processing', 'Machine learning'],\n",
        "    'label': [0, 1, 1]\n",
        "})\n",
        "print(f\"Created: {len(ds)} samples, columns: {ds.column_names}\")\n",
        "\n",
        "# Standard operations\n",
        "ds = ds.map(lambda x: {'text_len': len(x['text'])})     # Transform\n",
        "ds = ds.filter(lambda x: x['text_len'] > 10)            # Filter\n",
        "print(f\"After filter: {len(ds)} samples\")\n",
        "print(f\"First row: {ds[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert from Pandas\n",
        "import pandas as pd\n",
        "from data_juicer.core.data import NestedDataset\n",
        "\n",
        "df = pd.DataFrame({'text': ['From pandas!', 'Easy conversion']})\n",
        "ds = NestedDataset.from_pandas(df)\n",
        "print(f\"From Pandas: {ds['text']}\")\n",
        "\n",
        "# Convert back to Pandas\n",
        "df_back = ds.to_pandas()\n",
        "print(f\"Back to Pandas: {type(df_back)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data-Juicer Enhancements\n",
        "\n",
        "Beyond HuggingFace, `NestedDataset` adds:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Nested Field Access - use dot notation for nested structures\n",
        "from data_juicer.core.data import NestedDataset\n",
        "\n",
        "ds = NestedDataset.from_dict({\n",
        "    'text': ['Sample text', '中文样本'],\n",
        "    'meta': [{'source': 'wiki', 'date': '2024-01'}, {'source': 'web', 'date': '2024-02'}],\n",
        "    'stats': [{'lang': 'en', 'length': 100}, {'lang': 'zh', 'length': 20}]\n",
        "})\n",
        "\n",
        "# Access nested fields directly with dot notation\n",
        "print(f\"Source: {ds['meta.source']}\")  # No need for ds['meta'][0]['source']\n",
        "print(f\"Language: {ds['stats.lang']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Built-in Operator Pipeline - chain 100+ operators via .process()\n",
        "from data_juicer.core.data import NestedDataset\n",
        "from data_juicer.ops.filter import TextLengthFilter\n",
        "from data_juicer.ops.mapper import WhitespaceNormalizationMapper\n",
        "\n",
        "ds = NestedDataset.from_dict({\n",
        "    'text': [\n",
        "        'Short',\n",
        "        'This is a longer text that should pass the filter. aaaaaaaa',\n",
        "        'Text with various spaces'\n",
        "    ]\n",
        "})\n",
        "\n",
        "# Process with operator pipeline\n",
        "ds_processed = ds.process([\n",
        "    TextLengthFilter(min_len=10, max_len=30),      # Filter short texts\n",
        "    WhitespaceNormalizationMapper(),               # Whitespace normalization\n",
        "])\n",
        "\n",
        "print(f\"Before: {len(ds)} -> After: {len(ds_processed)} samples\")\n",
        "for row in ds_processed:\n",
        "    print(f\"  '{row['text']}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RayDataset (Distributed)\n",
        "\n",
        "`RayDataset` wraps Ray Data for distributed processing across multiple machines or GPUs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Direct usage: Create RayDataset from Ray Data\n",
        "import ray\n",
        "from data_juicer.core.data.ray_dataset import RayDataset\n",
        "\n",
        "# Initialize Ray\n",
        "ray.init(ignore_reinit_error=True)\n",
        "\n",
        "# Create Ray Data\n",
        "ray_data = ray.data.from_items([\n",
        "    {'text': 'Hello distributed world'},\n",
        "    {'text': 'Ray enables scalable processing'},\n",
        "    {'text': 'Data-Juicer on Ray'}\n",
        "])\n",
        "\n",
        "# Wrap in RayDataset\n",
        "ds = RayDataset(ray_data) # or dataset_path or cfg\n",
        "print(f\"Created RayDataset with {ds.count()} samples\")\n",
        "print(f\"First 2 samples: {ds.get(2)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Same operators work on RayDataset\n",
        "from data_juicer.ops.filter import TextLengthFilter\n",
        "\n",
        "# Process with operators - same API as NestedDataset\n",
        "ds_processed = ds.process([\n",
        "    TextLengthFilter(min_len=20)\n",
        "])\n",
        "\n",
        "print(f\"After filter: {ds_processed.count()} samples\")\n",
        "print(f\"Results: {ds_processed.get(10)}\")\n",
        "\n",
        "ray.shutdown()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Production Usage: Via Configuration\n",
        "\n",
        "For production, use configuration files with `executor_type: 'ray'`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a Ray config file\n",
        "import os\n",
        "os.makedirs('./configs', exist_ok=True)\n",
        "\n",
        "ray_config = \"\"\"\n",
        "project_name: 'ray-demo'\n",
        "dataset_path: './data/demo.jsonl'\n",
        "export_path: './outputs/processed'\n",
        "\n",
        "executor_type: 'ray'        # Enable Ray backend\n",
        "ray_address: 'auto'         # Or 'ray://hostname:port' for cluster\n",
        "\n",
        "process:\n",
        "  - text_length_filter:\n",
        "      min_len: 10\n",
        "      max_len: 1000\n",
        "\"\"\"\n",
        "\n",
        "with open('./configs/ray_demo.yaml', 'w') as f:\n",
        "    f.write(ray_config)\n",
        "\n",
        "print(\"Run with: dj-process --config ./configs/ray_demo.yaml\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Differences\n",
        "\n",
        "| Feature | NestedDataset | RayDataset |\n",
        "|---------|---------------|------------|\n",
        "| **Backend** | HuggingFace Dataset | Ray Data |\n",
        "| **Execution** | Eager | Lazy (streaming) |\n",
        "| **GPU Support** | Manual | Auto GPU allocation |\n",
        "| **Indexing** | `ds[0]`, `ds['col']` | `ds.get(k)`, `ds.get_column('col')` |\n",
        "\n",
        "See [Chapter 7: Distributed Processing with Ray](./07_Distributed_Processing_with_Ray.ipynb) for more details."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "data-juicer-hub",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
