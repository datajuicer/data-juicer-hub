{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 7: Pre-processing\n",
        "\n",
        "**Data-Juicer User Guide**\n",
        "\n",
        "- Git Commit: `v1.0.5`\n",
        "- Commit Date: 2026-01-16\n",
        "- Repository: https://github.com/datajuicer/data-juicer\n",
        "\n",
        "---\n",
        "\n",
        "Pre-processing tools help prepare raw data before entering the main Data-Juicer pipeline. See the [Preprocess README](https://datajuicer.github.io/data-juicer/en/main/tools/preprocess/README.html) for complete documentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Available Pre-processing Tools\n",
        "\n",
        "Data-Juicer provides several pre-processing utilities:\n",
        "\n",
        "- **dataset_split_by_language.py**: Split datasets by language\n",
        "- **raw_arxiv_to_jsonl.py**: Convert arXiv data to JSONL\n",
        "- **raw_stackexchange_to_jsonl.py**: Convert Stack Exchange data\n",
        "- **serialize_meta.py**: Serialize metadata fields"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone --depth 1 https://github.com/datajuicer/data-juicer.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!uv pip install -e data-juicer[dev]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example: Split Dataset by Language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# Create sample multilingual dataset\n",
        "os.makedirs('./data/raw', exist_ok=True)\n",
        "\n",
        "samples = [\n",
        "    {\"text\": \"This is an English text sample.\", \"id\": 1},\n",
        "    {\"text\": \"Ceci est un texte en français.\", \"id\": 2},\n",
        "    {\"text\": \"Another English sample for testing.\", \"id\": 3},\n",
        "    {\"text\": \"这是一个中文文本示例。\", \"id\": 4},\n",
        "    {\"text\": \"Machine learning is transforming industries.\", \"id\": 5},\n",
        "    {\"text\": \"Bonjour le monde!\", \"id\": 6}\n",
        "]\n",
        "\n",
        "with open('./data/raw/multilingual.jsonl', 'w') as f:\n",
        "    for sample in samples:\n",
        "        f.write(json.dumps(sample) + '\\n')\n",
        "\n",
        "print(f\"Created multilingual dataset with {len(samples)} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split by language\n",
        "!python data-juicer/tools/preprocess/dataset_split_by_language.py \\\n",
        "    --src_dir ./data/raw \\\n",
        "    --target_dir ./data/split_by_lang \\\n",
        "    --suffixes jsonl \\\n",
        "    --text_key text \\\n",
        "    --num_proc 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check split results\n",
        "import os\n",
        "import json\n",
        "\n",
        "split_dir = './data/split_by_lang'\n",
        "if os.path.exists(split_dir):\n",
        "    for filename in os.listdir(split_dir):\n",
        "        if filename.endswith('.jsonl'):\n",
        "            filepath = os.path.join(split_dir, filename)\n",
        "            with open(filepath, 'r') as f:\n",
        "                samples = [json.loads(line) for line in f]\n",
        "            print(f\"\\n{filename}: {len(samples)} samples\")\n",
        "            for sample in samples[:2]:  # Show first 2\n",
        "                print(f\"  - {sample['text'][:50]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example: Serialize Metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create dataset with complex metadata\n",
        "samples_with_meta = [\n",
        "    {\n",
        "        \"text\": \"Sample text one\",\n",
        "        \"meta\": {\n",
        "            \"source\": \"web\",\n",
        "            \"date\": \"2024-01-01\",\n",
        "            \"author\": \"user123\",\n",
        "            \"tags\": [\"tech\", \"ai\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"Sample text two\",\n",
        "        \"meta\": {\n",
        "            \"source\": \"social\",\n",
        "            \"date\": \"2024-01-02\",\n",
        "            \"author\": \"user456\",\n",
        "            \"tags\": [\"news\"]\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "os.makedirs('./data/with_meta', exist_ok=True)\n",
        "with open('./data/with_meta/data.jsonl', 'w') as f:\n",
        "    for sample in samples_with_meta:\n",
        "        f.write(json.dumps(sample) + '\\n')\n",
        "\n",
        "print(\"Created dataset with metadata\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Serialize metadata to string\n",
        "!python data-juicer/tools/preprocess/serialize_meta.py \\\n",
        "    --src_dir ./data/with_meta \\\n",
        "    --target_dir ./data/serialized \\\n",
        "    --text_key text \\\n",
        "    --serialized_key meta_str \\\n",
        "    --num_proc 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check serialized results\n",
        "with open('./data/serialized/data.jsonl', 'r') as f:\n",
        "    serialized = [json.loads(line) for line in f]\n",
        "\n",
        "print(\"Serialized metadata:\")\n",
        "for i, sample in enumerate(serialized, 1):\n",
        "    print(f\"\\n{i}. Text: {sample['text']}\")\n",
        "    print(f\"   Meta (serialized): {sample.get('meta_str', 'N/A')[:100]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Practical Workflow\n",
        "\n",
        "A typical pre-processing workflow:\n",
        "\n",
        "1. **Raw Data Collection**: Gather data from various sources\n",
        "2. **Format Conversion**: Convert to JSONL (if needed)\n",
        "3. **Language Splitting**: Separate by language for targeted processing\n",
        "4. **Metadata Handling**: Serialize complex metadata\n",
        "5. **Main Pipeline**: Feed into Data-Juicer processing pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Complete pre-processing + main pipeline\n",
        "config = \"\"\"project_name: 'preprocessed_data'\n",
        "dataset_path: './data/split_by_lang/en.jsonl'  # Use English split\n",
        "export_path: './outputs/preprocessed_final.jsonl'\n",
        "np: 2\n",
        "\n",
        "process:\n",
        "  - text_length_filter:\n",
        "      min_len: 10\n",
        "      max_len: 500\n",
        "  - alphanumeric_filter:\n",
        "      min_ratio: 0.5\n",
        "\"\"\"\n",
        "\n",
        "os.makedirs('./configs', exist_ok=True)\n",
        "with open('./configs/preprocess_pipeline.yaml', 'w') as f:\n",
        "    f.write(config)\n",
        "\n",
        "print(\"Pipeline config created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run main pipeline on pre-processed data\n",
        "!dj-process --config ./configs/preprocess_pipeline.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cleanup\n",
        "\n",
        "After completing the pre-processing tasks, clean up the cloned repository to save space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove cloned Data-Juicer repository\n",
        "!rm -rf data-juicer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Further Reading\n",
        "\n",
        "- [Pre-processing Tools Documentation](https://datajuicer.github.io/data-juicer/en/main/tools/preprocess/README.html)\n",
        "- [Pre-processing Scripts Source Code](https://github.com/datajuicer/data-juicer/blob/main/tools/preprocess/)\n",
        "- [Format Conversion Tools](https://datajuicer.github.io/data-juicer/en/main/tools/fmt_conversion/README.html)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "data-juicer-nk",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
