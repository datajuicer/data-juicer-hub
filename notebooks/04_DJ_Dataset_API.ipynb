{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 4: DJ Dataset API\n",
        "\n",
        "**Data-Juicer User Guide**\n",
        "\n",
        "- Git Commit: `v1.4.6`\n",
        "- Commit Date: 2026-02-02\n",
        "- Repository: https://github.com/datajuicer/data-juicer\n",
        "\n",
        "---\n",
        " \n",
        "> **Note:** This chapter is intended for users who want to programmatically call Data-Juicer or are familiar with HuggingFace Dataset operations.  \n",
        "> If you only care about YAML-based invocation, you can skip this chapter.\n",
        "\n",
        "Data-Juicer provides two dataset implementations:\n",
        "- **NestedDataset**: Built on HuggingFace Datasets, for single-machine processing\n",
        "- **RayDataset**: Built on Ray Data, for distributed processing\n",
        "\n",
        "Both share the same `DJDataset` interface, so you can switch backends without changing your operator code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table of Contents\n",
        "\n",
        "1. [Quick Comparison](#quick-comparison)\n",
        "2. [NestedDataset: HuggingFace-Compatible API](#nesteddataset-huggingface-compatible-api)\n",
        "3. [Data-Juicer Enhancements](#data-juicer-enhancements)\n",
        "4. [RayDataset (Distributed)](#raydataset-distributed)\n",
        "5. [Production Usage: Via Configuration](#production-usage-via-configuration)\n",
        "6. [Key Differences](#key-differences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Data-Juicer (if not installed)\n",
        "# If running in Google Colab, use 'pip install' instead of 'uv pip install'\n",
        "# !uv pip install py-data-juicer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick Comparison\n",
        "\n",
        "| Feature | Pandas | HuggingFace | Data-Juicer |\n",
        "|---------|--------|-------------|-------------|\n",
        "| **Base** | NumPy | Arrow | Built on HF |\n",
        "| **Indexing** | `df['col']` | `ds['col']` | Same + **nested access** (`ds['meta.source']`) |\n",
        "| **Processing** | `.apply()` | `.map()`, `.filter()` | Same + **100+ operators** via `.process()` |\n",
        "| **Multimodal** | Manual | Supported | **Lazy loading** for efficiency |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## NestedDataset: HuggingFace-Compatible API\n",
        "\n",
        "`NestedDataset` is fully compatible with HuggingFace Datasets API, so you can use familiar operations directly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/workspaces/data-juicer-hub/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "2026-02-12 09:24:50,255\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
            "2026-02-12 09:24:51,748\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created: 3 samples, columns: ['text', 'label']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 3/3 [00:00<00:00, 1138.42 examples/s]\n",
            "Filter: 100%|██████████| 3/3 [00:00<00:00, 1223.90 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After filter: 3 samples\n",
            "First row: {'text': 'Hello world', 'label': 0, 'text_len': 11}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# HuggingFace-style API works directly\n",
        "from data_juicer.core.data import NestedDataset\n",
        "\n",
        "# Create dataset (same as HuggingFace)\n",
        "ds = NestedDataset.from_dict({\n",
        "    'text': ['Hello world', 'Data processing', 'Machine learning'],\n",
        "    'label': [0, 1, 1]\n",
        "})\n",
        "print(f\"Created: {len(ds)} samples, columns: {ds.column_names}\")\n",
        "\n",
        "# Standard operations\n",
        "ds = ds.map(lambda x: {'text_len': len(x['text'])})     # Transform\n",
        "ds = ds.filter(lambda x: x['text_len'] > 10)            # Filter\n",
        "print(f\"After filter: {len(ds)} samples\")\n",
        "print(f\"First row: {ds[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "From Pandas: Column(['From pandas!', 'Easy conversion'])\n",
            "Back to Pandas: <class 'pandas.core.frame.DataFrame'>\n"
          ]
        }
      ],
      "source": [
        "# Convert from Pandas\n",
        "import pandas as pd\n",
        "from data_juicer.core.data import NestedDataset\n",
        "\n",
        "df = pd.DataFrame({'text': ['From pandas!', 'Easy conversion']})\n",
        "ds = NestedDataset.from_pandas(df)\n",
        "print(f\"From Pandas: {ds['text']}\")\n",
        "\n",
        "# Convert back to Pandas\n",
        "df_back = ds.to_pandas()\n",
        "print(f\"Back to Pandas: {type(df_back)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data-Juicer Enhancements\n",
        "\n",
        "Beyond HuggingFace, `NestedDataset` adds:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Source: Column(['wiki', 'web'])\n",
            "Language: Column(['en', 'zh'])\n"
          ]
        }
      ],
      "source": [
        "# 1. Nested Field Access - use dot notation for nested structures\n",
        "from data_juicer.core.data import NestedDataset\n",
        "\n",
        "ds = NestedDataset.from_dict({\n",
        "    'text': ['Sample text', '中文样本'],\n",
        "    'meta': [{'source': 'wiki', 'date': '2024-01'}, {'source': 'web', 'date': '2024-02'}],\n",
        "    'stats': [{'lang': 'en', 'length': 100}, {'lang': 'zh', 'length': 20}]\n",
        "})\n",
        "\n",
        "# Access nested fields directly with dot notation\n",
        "print(f\"Source: {ds['meta.source']}\")  # No need for ds['meta'][0]['source']\n",
        "print(f\"Language: {ds['stats.lang']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2026-02-12 09:24:51.936\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.utils.process_utils\u001b[0m:\u001b[36mcalculate_np\u001b[0m:\u001b[36m161\u001b[0m - \u001b[1mSet the auto `num_proc` to 4 of Op[text_length_filter] based on the required memory: NoneGB and required cpu: 1.\u001b[0m\n",
            "num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.\n",
            "WARNING:datasets.arrow_dataset:num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.\n",
            "\u001b[32m2026-02-12 09:24:51.940\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mdata_juicer.utils.resource_utils\u001b[0m:\u001b[36mquery_cuda_info\u001b[0m:\u001b[36m44\u001b[0m - \u001b[33m\u001b[1mCommand nvidia-smi is not found. There might be no GPUs on this machine.\u001b[0m\n",
            "Adding new column for stats (num_proc=3): 100%|██████████| 3/3 [00:00<00:00, 14.63 examples/s]\n",
            "\u001b[32m2026-02-12 09:24:52.159\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.utils.process_utils\u001b[0m:\u001b[36mcalculate_np\u001b[0m:\u001b[36m161\u001b[0m - \u001b[1mSet the auto `num_proc` to 4 of Op[text_length_filter] based on the required memory: NoneGB and required cpu: 1.\u001b[0m\n",
            "num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.\n",
            "WARNING:datasets.arrow_dataset:num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.\n",
            "text_length_filter_compute_stats (num_proc=3): 100%|██████████| 3/3 [00:00<00:00, 12.76 examples/s]\n",
            "\u001b[32m2026-02-12 09:24:52.434\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.utils.process_utils\u001b[0m:\u001b[36mcalculate_np\u001b[0m:\u001b[36m161\u001b[0m - \u001b[1mSet the auto `num_proc` to 4 of Op[text_length_filter] based on the required memory: NoneGB and required cpu: 1.\u001b[0m\n",
            "num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.\n",
            "WARNING:datasets.arrow_dataset:num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.\n",
            "text_length_filter_process (num_proc=3): 100%|██████████| 3/3 [00:00<00:00, 12.82 examples/s]\n",
            "\u001b[32m2026-02-12 09:24:52.983\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.core.data.dj_dataset\u001b[0m:\u001b[36mprocess\u001b[0m:\u001b[36m310\u001b[0m - \u001b[1m[1/2] OP [text_length_filter] Done in 1.085s. Left 1 samples.\u001b[0m\n",
            "\u001b[32m2026-02-12 09:24:53.018\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.utils.process_utils\u001b[0m:\u001b[36mcalculate_np\u001b[0m:\u001b[36m161\u001b[0m - \u001b[1mSet the auto `num_proc` to 4 of Op[whitespace_normalization_mapper] based on the required memory: NoneGB and required cpu: 1.\u001b[0m\n",
            "\u001b[32m2026-02-12 09:24:53.021\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mdata_juicer.utils.resource_utils\u001b[0m:\u001b[36mquery_cuda_info\u001b[0m:\u001b[36m44\u001b[0m - \u001b[33m\u001b[1mCommand nvidia-smi is not found. There might be no GPUs on this machine.\u001b[0m\n",
            "num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.\n",
            "WARNING:datasets.arrow_dataset:num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.\n",
            "whitespace_normalization_mapper_process (num_proc=1): 100%|██████████| 1/1 [00:00<00:00,  6.00 examples/s]\n",
            "\u001b[32m2026-02-12 09:24:53.565\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.core.data.dj_dataset\u001b[0m:\u001b[36mprocess\u001b[0m:\u001b[36m310\u001b[0m - \u001b[1m[2/2] OP [whitespace_normalization_mapper] Done in 0.582s. Left 1 samples.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before: 3 -> After: 1 samples\n",
            "  'Text with various spaces'\n"
          ]
        }
      ],
      "source": [
        "# 2. Built-in Operator Pipeline - chain 100+ operators via .process()\n",
        "from data_juicer.core.data import NestedDataset\n",
        "from data_juicer.ops.filter import TextLengthFilter\n",
        "from data_juicer.ops.mapper import WhitespaceNormalizationMapper\n",
        "\n",
        "ds = NestedDataset.from_dict({\n",
        "    'text': [\n",
        "        'Short',\n",
        "        'This is a longer text that should pass the filter. aaaaaaaa',\n",
        "        'Text with various spaces'\n",
        "    ]\n",
        "})\n",
        "\n",
        "# Process with operator pipeline\n",
        "ds_processed = ds.process([\n",
        "    TextLengthFilter(min_len=10, max_len=30),      # Filter short texts\n",
        "    WhitespaceNormalizationMapper(),               # Whitespace normalization\n",
        "])\n",
        "\n",
        "print(f\"Before: {len(ds)} -> After: {len(ds_processed)} samples\")\n",
        "for row in ds_processed:\n",
        "    print(f\"  '{row['text']}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RayDataset (Distributed)\n",
        "\n",
        "`RayDataset` wraps Ray Data for distributed processing across multiple machines or GPUs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-02-12 09:24:54,894\tWARNING services.py:2137 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67100672 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=4.13gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
            "2026-02-12 09:24:55,080\tINFO worker.py:2007 -- Started a local Ray instance.\n",
            "/workspaces/data-juicer-hub/.venv/lib/python3.11/site-packages/ray/_private/worker.py:2046: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
            "  warnings.warn(\n",
            "2026-02-12 09:24:56,803\tINFO dataset.py:3641 -- Tip: Use `take_batch()` instead of `take() / show()` to return records in pandas or numpy batch format.\n",
            "2026-02-12 09:24:56,833\tINFO logging.py:397 -- Registered dataset logger for dataset dataset_2_0\n",
            "2026-02-12 09:24:56,873\tINFO streaming_executor.py:178 -- Starting execution of Dataset dataset_2_0. Full logs are in /tmp/ray/session_2026-02-12_09-24-53_591542_10118/logs/ray-data\n",
            "2026-02-12 09:24:56,874\tINFO streaming_executor.py:179 -- Execution plan of Dataset dataset_2_0: InputDataBuffer[Input] -> LimitOperator[limit=2]\n",
            "2026-02-12 09:24:56,877\tINFO streaming_executor.py:686 -- [dataset]: A new progress UI is available. To enable, set `ray.data.DataContext.get_current().enable_rich_progress_bars = True` and `ray.data.DataContext.get_current().use_ray_tqdm = False`.\n",
            "2026-02-12 09:24:56,877\tINFO progress_bar.py:155 -- Progress bar disabled because stdout is a non-interactive terminal.\n",
            "2026-02-12 09:24:56,879\tWARNING resource_manager.py:136 -- ⚠️  Ray's object store is configured to use only 42.9% of available memory (3.8GiB out of 8.8GiB total). For optimal Ray Data performance, we recommend setting the object store to at least 50% of available memory. You can do this by setting the 'object_store_memory' parameter when calling ray.init() or by setting the RAY_DEFAULT_OBJECT_STORE_MEMORY_PROPORTION environment variable.\n",
            "2026-02-12 09:24:56,888\tINFO progress_bar.py:213 -- === Ray Data Progress {limit=2} ===\n",
            "2026-02-12 09:24:56,889\tINFO progress_bar.py:215 -- limit=2: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 62.0B object store: Progress Completed 0 / ?\n",
            "2026-02-12 09:24:56,890\tINFO progress_bar.py:213 -- === Ray Data Progress {Running Dataset} ===\n",
            "2026-02-12 09:24:56,891\tINFO progress_bar.py:215 -- Running Dataset: dataset_2_0. Active & requested resources: 0/4 CPU, 0.0B/1.9GiB object store: Progress Completed 0 / ?\n",
            "2026-02-12 09:24:56,901\tINFO streaming_executor.py:304 -- ✔️  Dataset dataset_2_0 execution finished in 0.03 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created RayDataset with 3 samples\n",
            "First 2 samples: [{'text': 'Hello distributed world'}, {'text': 'Ray enables scalable processing'}]\n"
          ]
        }
      ],
      "source": [
        "# Direct usage: Create RayDataset from Ray Data\n",
        "import ray\n",
        "from data_juicer.core.data.ray_dataset import RayDataset\n",
        "\n",
        "# Initialize Ray\n",
        "ray.init(ignore_reinit_error=True)\n",
        "\n",
        "# Create Ray Data\n",
        "ray_data = ray.data.from_items([\n",
        "    {'text': 'Hello distributed world'},\n",
        "    {'text': 'Ray enables scalable processing'},\n",
        "    {'text': 'Data-Juicer on Ray'}\n",
        "])\n",
        "\n",
        "# Wrap in RayDataset\n",
        "ds = RayDataset(ray_data) # or dataset_path or cfg\n",
        "print(f\"Created RayDataset with {ds.count()} samples\")\n",
        "print(f\"First 2 samples: {ds.get(2)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2026-02-12 09:24:56.952\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.utils.ray_utils\u001b[0m:\u001b[36mget_ray_nodes_info\u001b[0m:\u001b[36m96\u001b[0m - \u001b[1mRay nodes:\n",
            "[{'NodeID': 'f715a36d2a6cd173031c64580250e96fa5777b5353ce0ba0b74e4643', 'Alive': True, 'NodeManagerAddress': '10.0.0.151', 'NodeManagerHostname': 'codespaces-94212f', 'NodeManagerPort': 42675, 'ObjectManagerPort': 37905, 'ObjectStoreSocketName': '/tmp/ray/session_2026-02-12_09-24-53_591542_10118/sockets/plasma_store', 'RayletSocketName': '/tmp/ray/session_2026-02-12_09-24-53_591542_10118/sockets/raylet', 'MetricsExportPort': 62229, 'NodeName': '10.0.0.151', 'RuntimeEnvAgentPort': 61727, 'DeathReason': 0, 'DeathReasonMessage': '', 'alive': True, 'Resources': {'CPU': 4.0, 'object_store_memory': 4030113792.0, 'memory': 9403598848.0, 'node:10.0.0.151': 1.0, 'node:__internal_head__': 1.0}, 'Labels': {'ray.io/node-id': 'f715a36d2a6cd173031c64580250e96fa5777b5353ce0ba0b74e4643'}}]\u001b[0m\n",
            "\u001b[32m2026-02-12 09:24:57.084\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.utils.ray_utils\u001b[0m:\u001b[36mget_ray_nodes_info\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mRay cluster info:\n",
            "{'f715a36d2a6cd173031c64580250e96fa5777b5353ce0ba0b74e4643': {'free_memory': 11015, 'cpu_count': 4, 'gpu_count': 0, 'gpus_memory': [], 'free_gpus_memory': []}}\u001b[0m\n",
            "\u001b[32m2026-02-12 09:24:57.086\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mdata_juicer.utils.process_utils\u001b[0m:\u001b[36mcalculate_ray_np\u001b[0m:\u001b[36m365\u001b[0m - \u001b[33m\u001b[1mNeither the required memory nor cpu of Op[text_length_filter] is specified. We recommend specifying the `num_cpus` field in the config file. You can reference data_juicer/config/config_all.yaml.\u001b[0m\n",
            "\u001b[32m2026-02-12 09:24:57.086\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.utils.process_utils\u001b[0m:\u001b[36mcalculate_ray_np\u001b[0m:\u001b[36m532\u001b[0m - \u001b[1mOp[text_length_filter] will be executed with the following resources: num_cpus: None, num_gpus: None, concurrency: 4, \u001b[0m\n",
            "2026-02-12 09:24:57,090\tWARNING util.py:599 -- The argument ``concurrency`` is deprecated in Ray 2.51. Please specify argument ``compute`` instead. For more information, see https://docs.ray.io/en/master/data/transforming-data.html#stateful-transforms.\n",
            "2026-02-12 09:24:57,094\tINFO logging.py:397 -- Registered dataset logger for dataset dataset_6_0\n",
            "2026-02-12 09:24:57,097\tINFO streaming_executor.py:178 -- Starting execution of Dataset dataset_6_0. Full logs are in /tmp/ray/session_2026-02-12_09-24-53_591542_10118/logs/ray-data\n",
            "2026-02-12 09:24:57,098\tINFO streaming_executor.py:179 -- Execution plan of Dataset dataset_6_0: InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(process_batch_arrow)] -> TaskPoolMapOperator[MapBatches(compute_stats_batched)] -> TaskPoolMapOperator[MapBatches(filter_batch)] -> TaskPoolMapOperator[Project] -> AggregateNumRows[AggregateNumRows]\n",
            "2026-02-12 09:24:57,116\tINFO progress_bar.py:213 -- === Ray Data Progress {MapBatches(process_batch_arrow)} ===\n",
            "2026-02-12 09:24:57,117\tINFO progress_bar.py:215 -- MapBatches(process_batch_arrow): Tasks: 1 [backpressured:tasks]; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
            "2026-02-12 09:24:57,118\tINFO progress_bar.py:213 -- === Ray Data Progress {MapBatches(compute_stats_batched)} ===\n",
            "2026-02-12 09:24:57,119\tINFO progress_bar.py:215 -- MapBatches(compute_stats_batched): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
            "2026-02-12 09:24:57,120\tINFO progress_bar.py:213 -- === Ray Data Progress {MapBatches(filter_batch)} ===\n",
            "2026-02-12 09:24:57,120\tINFO progress_bar.py:215 -- MapBatches(filter_batch): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
            "2026-02-12 09:24:57,121\tINFO progress_bar.py:213 -- === Ray Data Progress {Project} ===\n",
            "2026-02-12 09:24:57,121\tINFO progress_bar.py:215 -- Project: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
            "2026-02-12 09:24:57,122\tINFO progress_bar.py:213 -- === Ray Data Progress {AggregateNumRows} ===\n",
            "2026-02-12 09:24:57,122\tINFO progress_bar.py:215 -- AggregateNumRows: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / 1\n",
            "2026-02-12 09:24:57,123\tINFO progress_bar.py:213 -- === Ray Data Progress {Running Dataset} ===\n",
            "2026-02-12 09:24:57,123\tINFO progress_bar.py:215 -- Running Dataset: dataset_6_0. Active & requested resources: 0/4 CPU, 0.0B/1.9GiB object store: Progress Completed 0 / 1\n",
            "/workspaces/data-juicer-hub/.venv/lib/python3.11/site-packages/ray/data/_internal/execution/operators/task_pool_map_operator.py:178: UserWarning: The maximum number of concurrent tasks for 'MapBatches(compute_stats_batched)' is set to 4, but the operator only received 1 input(s). This means that the operator can launch at most 1 task(s), which is less than the concurrency limit. You might be able to increase the number of concurrent tasks by configuring `override_num_blocks` earlier in the pipeline.\n",
            "  warnings.warn(\n",
            "2026-02-12 09:25:02,192\tINFO progress_bar.py:215 -- MapBatches(process_batch_arrow): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 84.0B object store: Progress Completed 3 / ?\n",
            "2026-02-12 09:25:02,193\tINFO progress_bar.py:215 -- MapBatches(compute_stats_batched): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 216.0B object store: Progress Completed 3 / ?\n",
            "2026-02-12 09:25:02,193\tINFO progress_bar.py:215 -- MapBatches(filter_batch): Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
            "2026-02-12 09:25:02,194\tINFO progress_bar.py:215 -- Project: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
            "2026-02-12 09:25:02,195\tINFO progress_bar.py:215 -- AggregateNumRows: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / 1\n",
            "2026-02-12 09:25:02,195\tINFO progress_bar.py:215 -- Running Dataset: dataset_6_0. Active & requested resources: 1/4 CPU, 384.0MiB/1.9GiB object store: Progress Completed 0 / 1\n",
            "2026-02-12 09:25:02,384\tINFO streaming_executor.py:304 -- ✔️  Dataset dataset_6_0 execution finished in 5.29 seconds\n",
            "2026-02-12 09:25:02,392\tINFO logging.py:397 -- Registered dataset logger for dataset dataset_7_0\n",
            "2026-02-12 09:25:02,395\tINFO streaming_executor.py:178 -- Starting execution of Dataset dataset_7_0. Full logs are in /tmp/ray/session_2026-02-12_09-24-53_591542_10118/logs/ray-data\n",
            "2026-02-12 09:25:02,396\tINFO streaming_executor.py:179 -- Execution plan of Dataset dataset_7_0: InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(process_batch_arrow)] -> TaskPoolMapOperator[MapBatches(compute_stats_batched)] -> TaskPoolMapOperator[MapBatches(filter_batch)] -> TaskPoolMapOperator[Project] -> AggregateNumRows[AggregateNumRows]\n",
            "2026-02-12 09:25:02,410\tINFO progress_bar.py:213 -- === Ray Data Progress {MapBatches(process_batch_arrow)} ===\n",
            "2026-02-12 09:25:02,411\tINFO progress_bar.py:215 -- MapBatches(process_batch_arrow): Tasks: 1 [backpressured:tasks]; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
            "2026-02-12 09:25:02,411\tINFO progress_bar.py:213 -- === Ray Data Progress {MapBatches(compute_stats_batched)} ===\n",
            "2026-02-12 09:25:02,412\tINFO progress_bar.py:215 -- MapBatches(compute_stats_batched): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
            "2026-02-12 09:25:02,413\tINFO progress_bar.py:213 -- === Ray Data Progress {MapBatches(filter_batch)} ===\n",
            "2026-02-12 09:25:02,414\tINFO progress_bar.py:215 -- MapBatches(filter_batch): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
            "2026-02-12 09:25:02,414\tINFO progress_bar.py:213 -- === Ray Data Progress {Project} ===\n",
            "2026-02-12 09:25:02,415\tINFO progress_bar.py:215 -- Project: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
            "2026-02-12 09:25:02,415\tINFO progress_bar.py:213 -- === Ray Data Progress {AggregateNumRows} ===\n",
            "2026-02-12 09:25:02,417\tINFO progress_bar.py:215 -- AggregateNumRows: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / 1\n",
            "2026-02-12 09:25:02,417\tINFO progress_bar.py:213 -- === Ray Data Progress {Running Dataset} ===\n",
            "2026-02-12 09:25:02,417\tINFO progress_bar.py:215 -- Running Dataset: dataset_7_0. Active & requested resources: 0/4 CPU, 0.0B/1.9GiB object store: Progress Completed 0 / 1\n",
            "2026-02-12 09:25:02,472\tINFO streaming_executor.py:304 -- ✔️  Dataset dataset_7_0 execution finished in 0.08 seconds\n",
            "2026-02-12 09:25:02,480\tINFO logging.py:397 -- Registered dataset logger for dataset dataset_9_0\n",
            "2026-02-12 09:25:02,481\tINFO limit_pushdown.py:140 -- Skipping push down of limit 2 through map MapBatches[MapBatches(filter_batch)] because it requires 1000 rows to produce stable outputs\n",
            "2026-02-12 09:25:02,482\tINFO limit_pushdown.py:140 -- Skipping push down of limit 2 through map MapBatches[MapBatches(filter_batch)] because it requires 1000 rows to produce stable outputs\n",
            "2026-02-12 09:25:02,483\tINFO streaming_executor.py:178 -- Starting execution of Dataset dataset_9_0. Full logs are in /tmp/ray/session_2026-02-12_09-24-53_591542_10118/logs/ray-data\n",
            "2026-02-12 09:25:02,483\tINFO streaming_executor.py:179 -- Execution plan of Dataset dataset_9_0: InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(process_batch_arrow)] -> TaskPoolMapOperator[MapBatches(compute_stats_batched)] -> TaskPoolMapOperator[MapBatches(filter_batch)] -> LimitOperator[limit=2]\n",
            "2026-02-12 09:25:02,495\tINFO progress_bar.py:213 -- === Ray Data Progress {MapBatches(process_batch_arrow)} ===\n",
            "2026-02-12 09:25:02,497\tINFO progress_bar.py:215 -- MapBatches(process_batch_arrow): Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
            "2026-02-12 09:25:02,497\tINFO progress_bar.py:213 -- === Ray Data Progress {MapBatches(compute_stats_batched)} ===\n",
            "2026-02-12 09:25:02,498\tINFO progress_bar.py:215 -- MapBatches(compute_stats_batched): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
            "2026-02-12 09:25:02,498\tINFO progress_bar.py:213 -- === Ray Data Progress {MapBatches(filter_batch)} ===\n",
            "2026-02-12 09:25:02,499\tINFO progress_bar.py:215 -- MapBatches(filter_batch): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
            "2026-02-12 09:25:02,500\tINFO progress_bar.py:213 -- === Ray Data Progress {limit=2} ===\n",
            "2026-02-12 09:25:02,501\tINFO progress_bar.py:215 -- limit=2: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
            "2026-02-12 09:25:02,502\tINFO progress_bar.py:213 -- === Ray Data Progress {Running Dataset} ===\n",
            "2026-02-12 09:25:02,503\tINFO progress_bar.py:215 -- Running Dataset: dataset_9_0. Active & requested resources: 0/4 CPU, 0.0B/1.9GiB object store: Progress Completed 0 / ?\n",
            "2026-02-12 09:25:02,547\tINFO streaming_executor.py:304 -- ✔️  Dataset dataset_9_0 execution finished in 0.06 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After filter: 2 samples\n",
            "Results: [{'text': 'Hello distributed world', '__dj__stats__': {'text_len': 23}}, {'text': 'Ray enables scalable processing', '__dj__stats__': {'text_len': 31}}]\n"
          ]
        }
      ],
      "source": [
        "# Same operators work on RayDataset\n",
        "from data_juicer.ops.filter import TextLengthFilter\n",
        "\n",
        "# Process with operators - same API as NestedDataset\n",
        "ds_processed = ds.process([\n",
        "    TextLengthFilter(min_len=20)\n",
        "])\n",
        "\n",
        "print(f\"After filter: {ds_processed.count()} samples\")\n",
        "print(f\"Results: {ds_processed.get(10)}\")\n",
        "\n",
        "ray.shutdown()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Production Usage: Via Configuration\n",
        "\n",
        "For production, use configuration files with `executor_type: 'ray'`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Run with: dj-process --config ./configs/ray_demo.yaml\n"
          ]
        }
      ],
      "source": [
        "# Create a Ray config file\n",
        "import os\n",
        "os.makedirs('./configs', exist_ok=True)\n",
        "\n",
        "ray_config = \"\"\"\n",
        "project_name: 'ray-demo'\n",
        "dataset_path: './data/demo.jsonl'\n",
        "export_path: './outputs/processed'\n",
        "\n",
        "executor_type: 'ray'        # Enable Ray backend\n",
        "ray_address: 'auto'         # Or 'ray://hostname:port' for cluster\n",
        "\n",
        "process:\n",
        "  - text_length_filter:\n",
        "      min_len: 10\n",
        "      max_len: 1000\n",
        "\"\"\"\n",
        "\n",
        "with open('./configs/ray_demo.yaml', 'w') as f:\n",
        "    f.write(ray_config)\n",
        "\n",
        "print(\"Run with: dj-process --config ./configs/ray_demo.yaml\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Differences\n",
        "\n",
        "| Feature | NestedDataset | RayDataset |\n",
        "|---------|---------------|------------|\n",
        "| **Backend** | HuggingFace Dataset | Ray Data |\n",
        "| **Execution** | Eager | Lazy (streaming) |\n",
        "| **GPU Support** | Manual | Auto GPU allocation |\n",
        "| **Indexing** | `ds[0]`, `ds['col']` | `ds.get(k)`, `ds.get_column('col')` |\n",
        "\n",
        "See [Chapter 7: Distributed Processing with Ray](./07_Distributed_Processing_with_Ray.ipynb) for more details."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "data-juicer-hub (3.11.14)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
