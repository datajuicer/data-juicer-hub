{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Chapter 5: Operators Usage\n",
                "\n",
                "**Data-Juicer User Guide**\n",
                "\n",
                "- Git Commit: `v1.4.6`\n",
                "- Commit Date: 2026-02-02\n",
                "- Repository: https://github.com/datajuicer/data-juicer\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Operators are the core building blocks of Data-Juicer pipelines. This chapter demonstrates how to use operators programmatically through the Python API.\n",
                "\n",
                "There are two primary ways to use operators:\n",
                "\n",
                "1. **YAML Configuration** (declarative): Define your pipeline in a YAML file and execute it with the CLI\n",
                "2. **Python API** (programmatic): Instantiate and chain operators directly in Python code\n",
                "\n",
                "Both approaches offer flexibility—choose based on your workflow preferences.\n",
                "\n",
                "**Note:** For a complete list of operators and their parameters, refer to the [Operators Documentation](https://datajuicer.github.io/data-juicer/en/main/docs/Operators.html)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Table of Contents\n",
                "\n",
                "1. [YAML-Based Configuration](#yaml-based-configuration)\n",
                "2. [Operator Types](#operator-types)\n",
                "3. [Setup](#setup)\n",
                "4. [Create Sample Dataset](#create-sample-dataset)\n",
                "5. [Initialize and Call a Single Operator](#initialize-and-call-a-single-operator)\n",
                "6. [Chain Multiple Operators Sequentially](#chain-multiple-operators-sequentially)\n",
                "7. [Batch Processing with Operator List](#batch-processing-with-operator-list)\n",
                "8. [Inspect Operator Statistics](#inspect-operator-statistics)\n",
                "9.  [Further Reading](#further-reading)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## YAML-Based Configuration\n",
                "\n",
                "For declarative configuration, define your operator pipeline in a YAML file:\n",
                "\n",
                "```yaml\n",
                "project_name: 'operators_demo'\n",
                "dataset_path: './data/operators_demo.jsonl'\n",
                "export_path: './outputs/operators_demo.jsonl'\n",
                "np: 1\n",
                "\n",
                "process:\n",
                "  - whitespace_normalization_mapper: {}\n",
                "  - clean_email_mapper: {}\n",
                "  - language_id_score_filter:\n",
                "      lang: 'en'\n",
                "      min_score: 0.8\n",
                "  - text_length_filter:\n",
                "      min_len: 20\n",
                "      max_len: 200\n",
                "  - alphanumeric_filter:\n",
                "      min_ratio: 0.5\n",
                "```\n",
                "\n",
                "Execute the configuration file using the following command:\n",
                "\n",
                "```bash\n",
                "dj-process --config config.yaml\n",
                "```\n",
                "\n",
                "For detailed guidance on creating and using recipe YAML files, please refer to [Building Recipes](./02_Building_Recipes.ipynb)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Operator Types\n",
                "\n",
                "Data-Juicer provides several operator categories:\n",
                "\n",
                "| Operator Type | Purpose | Examples |\n",
                "|---|---|---|\n",
                "| **Mapper** | Edits and transforms samples. | `CleanEmailMapper`, `WhitespaceNormalizationMapper` |\n",
                "| **Filter** | Removes low-quality samples based on criteria | `LanguageIDScoreFilter`, `TextLengthFilter`, `AlphanumericFilter` |\n",
                "| **Deduplicator** | Detects and removes duplicate samples. | `DocumentDeduplicator`, `ImageDeduplicator` |\n",
                "| **Selector** | Selects top samples based on ranking. | `TopkSpecifiedFieldSelector` |\n",
                "| **Grouper** | Group samples to batched samples. | `KeyValueGrouper` |\n",
                "| **Aggregator** | \tAggregate for batched samples, such as summary or conclusion. | `MetaTagsAggregator` |\n",
                "| **Pipeline** | Applies dataset-level processing; both input and output are datasets. | `RayVLLMEnginePipeline` |\n",
                "\n",
                "Each operator can be configured with specific parameters to suit your data processing requirements."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install Data-Juicer (if not installed)\n",
                "# If running in Google Colab, use 'pip install' instead of 'uv pip install'\n",
                "# !uv pip install py-data-juicer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/workspaces/data-juicer-hub/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n",
                        "2026-02-12 09:27:39,822\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
                        "2026-02-12 09:27:41,385\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
                    ]
                }
            ],
            "source": [
                "from data_juicer.core.data import NestedDataset as Dataset\n",
                "from data_juicer.ops.filter import LanguageIDScoreFilter, TextLengthFilter, AlphanumericFilter\n",
                "from data_juicer.ops.mapper import CleanEmailMapper, WhitespaceNormalizationMapper"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Create Sample Dataset\n",
                "\n",
                "We'll create a sample dataset with varied data quality to demonstrate how different operators handle various scenarios."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Created dataset with 7 samples\n"
                    ]
                }
            ],
            "source": [
                "# Sample data with various quality levels\n",
                "samples = [\n",
                "    {\"text\": \"This is a high-quality English text sample.\"},\n",
                "    {\"text\": \"Short\"},\n",
                "    {\"text\": \"Contact us at support@example.com for more information.\"},\n",
                "    {\"text\": \"Bonjour! Ceci est un texte en français.\"},\n",
                "    {\"text\": \"Machine learning is transforming industries worldwide.\"},\n",
                "    {\"text\": \"a@#$%^&*()_+{}[]|\\\\:;<>?,./\"},\n",
                "    {\"text\": \"This has\\textra\twhitespace　issues.\"}\n",
                "]\n",
                "\n",
                "# Create Dataset object\n",
                "dataset = Dataset.from_list(samples)\n",
                "print(f\"Created dataset with {len(dataset)} samples\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Initialize and Call a Single Operator\n",
                "\n",
                "Start by applying a single operator to understand how they work. Here we use `LanguageIDScoreFilter` to keep only English texts above a confidence threshold."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[32m2026-02-12 09:27:51.467\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.utils.model_utils\u001b[0m:\u001b[36mprepare_fasttext_model\u001b[0m:\u001b[36m502\u001b[0m - \u001b[1mLoading fasttext language identification model...\u001b[0m\n",
                        "\u001b[32m2026-02-12 09:27:51.567\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.utils.process_utils\u001b[0m:\u001b[36mcalculate_np\u001b[0m:\u001b[36m161\u001b[0m - \u001b[1mSet the auto `num_proc` to 4 of Op[language_id_score_filter] based on the required memory: NoneGB and required cpu: 1.\u001b[0m\n",
                        "Adding new column for stats (num_proc=4): 100%|██████████| 7/7 [00:00<00:00, 28.57 examples/s]\n",
                        "\u001b[32m2026-02-12 09:27:51.824\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.utils.process_utils\u001b[0m:\u001b[36mcalculate_np\u001b[0m:\u001b[36m161\u001b[0m - \u001b[1mSet the auto `num_proc` to 4 of Op[language_id_score_filter] based on the required memory: NoneGB and required cpu: 1.\u001b[0m\n",
                        "language_id_score_filter_compute_stats (num_proc=4):   0%|          | 0/7 [00:00<?, ? examples/s]\u001b[32m2026-02-12 09:27:51.941\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.utils.model_utils\u001b[0m:\u001b[36mprepare_fasttext_model\u001b[0m:\u001b[36m502\u001b[0m - \u001b[1mLoading fasttext language identification model...\u001b[0m\n",
                        "\u001b[32m2026-02-12 09:27:51.947\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.utils.model_utils\u001b[0m:\u001b[36mprepare_fasttext_model\u001b[0m:\u001b[36m502\u001b[0m - \u001b[1mLoading fasttext language identification model...\u001b[0m\n",
                        "\u001b[32m2026-02-12 09:27:51.957\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.utils.model_utils\u001b[0m:\u001b[36mprepare_fasttext_model\u001b[0m:\u001b[36m502\u001b[0m - \u001b[1mLoading fasttext language identification model...\u001b[0m\n",
                        "\u001b[32m2026-02-12 09:27:51.963\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.utils.model_utils\u001b[0m:\u001b[36mprepare_fasttext_model\u001b[0m:\u001b[36m502\u001b[0m - \u001b[1mLoading fasttext language identification model...\u001b[0m\n",
                        "language_id_score_filter_compute_stats (num_proc=4): 100%|██████████| 7/7 [00:00<00:00, 13.90 examples/s]\n",
                        "\u001b[32m2026-02-12 09:27:52.348\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.utils.process_utils\u001b[0m:\u001b[36mcalculate_np\u001b[0m:\u001b[36m161\u001b[0m - \u001b[1mSet the auto `num_proc` to 4 of Op[language_id_score_filter] based on the required memory: NoneGB and required cpu: 1.\u001b[0m\n",
                        "language_id_score_filter_process (num_proc=4): 100%|██████████| 7/7 [00:00<00:00, 26.67 examples/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Original: 7 samples\n",
                        "After language filter: 5 samples\n",
                        "\n",
                        "Filtered samples:\n",
                        "1. This is a high-quality English text sample.\n",
                        "2. Short\n",
                        "3. Contact us at support@example.com for more information.\n",
                        "4. Machine learning is transforming industries worldwide.\n",
                        "5. This has\textra\twhitespace　issues.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Initialize LanguageIDScoreFilter\n",
                "lang_filter = LanguageIDScoreFilter(\n",
                "    lang='en',      # Keep English samples\n",
                "    min_score=0.6   # Minimum confidence score\n",
                ")\n",
                "\n",
                "# Apply the filter\n",
                "filtered_dataset = lang_filter.run(dataset)\n",
                "\n",
                "print(f\"Original: {len(dataset)} samples\")\n",
                "print(f\"After language filter: {len(filtered_dataset)} samples\")\n",
                "print(\"\\nFiltered samples:\")\n",
                "for i, sample in enumerate(filtered_dataset, 1):\n",
                "    print(f\"{i}. {sample['text']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Chain Multiple Operators Sequentially\n",
                "\n",
                "In practice, you'll often want to apply multiple operators in sequence. This approach gives you fine-grained control over the pipeline and allows you to inspect intermediate results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[32m2026-02-12 09:27:57.888\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.utils.process_utils\u001b[0m:\u001b[36mcalculate_np\u001b[0m:\u001b[36m161\u001b[0m - \u001b[1mSet the auto `num_proc` to 4 of Op[whitespace_normalization_mapper] based on the required memory: NoneGB and required cpu: 1.\u001b[0m\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Step 1: Normalizing whitespace...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "whitespace_normalization_mapper_process (num_proc=4): 100%|██████████| 7/7 [00:00<00:00, 27.54 examples/s]\n",
                        "\u001b[32m2026-02-12 09:27:58.161\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.utils.model_utils\u001b[0m:\u001b[36mprepare_fasttext_model\u001b[0m:\u001b[36m502\u001b[0m - \u001b[1mLoading fasttext language identification model...\u001b[0m\n",
                        "\u001b[32m2026-02-12 09:27:58.250\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.utils.process_utils\u001b[0m:\u001b[36mcalculate_np\u001b[0m:\u001b[36m161\u001b[0m - \u001b[1mSet the auto `num_proc` to 4 of Op[language_id_score_filter] based on the required memory: NoneGB and required cpu: 1.\u001b[0m\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  → 7 samples\n",
                        "Step 2: Filtering by language (English, min_score=0.6)...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Adding new column for stats (num_proc=4): 100%|██████████| 7/7 [00:00<00:00, 34.12 examples/s]\n",
                        "\u001b[32m2026-02-12 09:27:58.467\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.utils.process_utils\u001b[0m:\u001b[36mcalculate_np\u001b[0m:\u001b[36m161\u001b[0m - \u001b[1mSet the auto `num_proc` to 4 of Op[language_id_score_filter] based on the required memory: NoneGB and required cpu: 1.\u001b[0m\n",
                        "language_id_score_filter_compute_stats (num_proc=4):   0%|          | 0/7 [00:00<?, ? examples/s]\u001b[32m2026-02-12 09:27:58.655\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.utils.model_utils\u001b[0m:\u001b[36mprepare_fasttext_model\u001b[0m:\u001b[36m502\u001b[0m - \u001b[1mLoading fasttext language identification model...\u001b[0m\n",
                        "\u001b[32m2026-02-12 09:27:58.655\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.utils.model_utils\u001b[0m:\u001b[36mprepare_fasttext_model\u001b[0m:\u001b[36m502\u001b[0m - \u001b[1mLoading fasttext language identification model...\u001b[0m\n",
                        "\u001b[32m2026-02-12 09:27:58.660\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.utils.model_utils\u001b[0m:\u001b[36mprepare_fasttext_model\u001b[0m:\u001b[36m502\u001b[0m - \u001b[1mLoading fasttext language identification model...\u001b[0m\n",
                        "\u001b[32m2026-02-12 09:27:58.668\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.utils.model_utils\u001b[0m:\u001b[36mprepare_fasttext_model\u001b[0m:\u001b[36m502\u001b[0m - \u001b[1mLoading fasttext language identification model...\u001b[0m\n",
                        "language_id_score_filter_compute_stats (num_proc=4): 100%|██████████| 7/7 [00:00<00:00, 15.34 examples/s]\n",
                        "\u001b[32m2026-02-12 09:27:58.936\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.utils.process_utils\u001b[0m:\u001b[36mcalculate_np\u001b[0m:\u001b[36m161\u001b[0m - \u001b[1mSet the auto `num_proc` to 4 of Op[language_id_score_filter] based on the required memory: NoneGB and required cpu: 1.\u001b[0m\n",
                        "language_id_score_filter_process (num_proc=4): 100%|██████████| 7/7 [00:00<00:00, 28.13 examples/s]\n",
                        "\u001b[32m2026-02-12 09:27:59.200\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.utils.process_utils\u001b[0m:\u001b[36mcalculate_np\u001b[0m:\u001b[36m161\u001b[0m - \u001b[1mSet the auto `num_proc` to 4 of Op[text_length_filter] based on the required memory: NoneGB and required cpu: 1.\u001b[0m\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  → 5 samples\n",
                        "Step 3: Filtering by text length (20-200 chars)...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "text_length_filter_compute_stats (num_proc=4): 100%|██████████| 5/5 [00:00<00:00, 16.45 examples/s]\n",
                        "\u001b[32m2026-02-12 09:27:59.534\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.utils.process_utils\u001b[0m:\u001b[36mcalculate_np\u001b[0m:\u001b[36m161\u001b[0m - \u001b[1mSet the auto `num_proc` to 4 of Op[text_length_filter] based on the required memory: NoneGB and required cpu: 1.\u001b[0m\n",
                        "text_length_filter_process (num_proc=4): 100%|██████████| 5/5 [00:00<00:00, 16.66 examples/s]\n",
                        "\u001b[32m2026-02-12 09:27:59.875\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.utils.process_utils\u001b[0m:\u001b[36mcalculate_np\u001b[0m:\u001b[36m161\u001b[0m - \u001b[1mSet the auto `num_proc` to 4 of Op[alphanumeric_filter] based on the required memory: NoneGB and required cpu: 1.\u001b[0m\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  → 4 samples\n",
                        "Step 4: Filtering by alphanumeric ratio (min=0.5)...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "alphanumeric_filter_compute_stats (num_proc=4): 100%|██████████| 4/4 [00:00<00:00, 12.12 examples/s]\n",
                        "\u001b[32m2026-02-12 09:28:00.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.utils.process_utils\u001b[0m:\u001b[36mcalculate_np\u001b[0m:\u001b[36m161\u001b[0m - \u001b[1mSet the auto `num_proc` to 4 of Op[alphanumeric_filter] based on the required memory: NoneGB and required cpu: 1.\u001b[0m\n",
                        "alphanumeric_filter_process (num_proc=4): 100%|██████████| 4/4 [00:00<00:00, 14.96 examples/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  → 4 samples\n",
                        "\n",
                        "Final output:\n",
                        "1. This is a high-quality English text sample.\n",
                        "2. Contact us at support@example.com for more information.\n",
                        "3. Machine learning is transforming industries worldwide.\n",
                        "4. This has extra whitespace issues.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Method 1: Sequential application\n",
                "dataset = Dataset.from_list(samples)\n",
                "\n",
                "# Step 1: Normalize whitespace\n",
                "print(\"Step 1: Normalizing whitespace...\")\n",
                "whitespace_mapper = WhitespaceNormalizationMapper()\n",
                "dataset = whitespace_mapper.run(dataset)\n",
                "print(f\"  → {len(dataset)} samples\")\n",
                "\n",
                "# Step 2: Filter by language\n",
                "print(\"Step 2: Filtering by language (English, min_score=0.6)...\")\n",
                "lang_filter = LanguageIDScoreFilter(lang='en', min_score=0.6)\n",
                "dataset = lang_filter.run(dataset)\n",
                "print(f\"  → {len(dataset)} samples\")\n",
                "\n",
                "# Step 3: Filter by text length\n",
                "print(\"Step 3: Filtering by text length (20-200 chars)...\")\n",
                "length_filter = TextLengthFilter(min_len=20, max_len=200)\n",
                "dataset = length_filter.run(dataset)\n",
                "print(f\"  → {len(dataset)} samples\")\n",
                "\n",
                "# Step 4: Filter by alphanumeric ratio\n",
                "print(\"Step 4: Filtering by alphanumeric ratio (min=0.5)...\")\n",
                "alpha_filter = AlphanumericFilter(min_ratio=0.5)\n",
                "dataset = alpha_filter.run(dataset)\n",
                "print(f\"  → {len(dataset)} samples\")\n",
                "\n",
                "print(\"\\nFinal output:\")\n",
                "for i, sample in enumerate(dataset, 1):\n",
                "    print(f\"{i}. {sample['text']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Batch Processing with Operator List\n",
                "\n",
                "For cleaner code and better performance, you can pass all operators to the `process()` method at once."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[32m2026-02-12 09:28:07.136\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.utils.model_utils\u001b[0m:\u001b[36mprepare_fasttext_model\u001b[0m:\u001b[36m502\u001b[0m - \u001b[1mLoading fasttext language identification model...\u001b[0m\n",
                        "\u001b[32m2026-02-12 09:28:07.310\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.utils.process_utils\u001b[0m:\u001b[36mcalculate_np\u001b[0m:\u001b[36m161\u001b[0m - \u001b[1mSet the auto `num_proc` to 4 of Op[whitespace_normalization_mapper] based on the required memory: NoneGB and required cpu: 1.\u001b[0m\n",
                        "\u001b[32m2026-02-12 09:28:07.323\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mdata_juicer.utils.resource_utils\u001b[0m:\u001b[36mquery_cuda_info\u001b[0m:\u001b[36m44\u001b[0m - \u001b[33m\u001b[1mCommand nvidia-smi is not found. There might be no GPUs on this machine.\u001b[0m\n",
                        "whitespace_normalization_mapper_process (num_proc=4): 100%|██████████| 7/7 [00:00<00:00, 25.92 examples/s]\n",
                        "\u001b[32m2026-02-12 09:28:07.881\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.core.data.dj_dataset\u001b[0m:\u001b[36mprocess\u001b[0m:\u001b[36m310\u001b[0m - \u001b[1m[1/5] OP [whitespace_normalization_mapper] Done in 0.628s. Left 7 samples.\u001b[0m\n",
                        "\u001b[32m2026-02-12 09:28:07.919\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.utils.process_utils\u001b[0m:\u001b[36mcalculate_np\u001b[0m:\u001b[36m161\u001b[0m - \u001b[1mSet the auto `num_proc` to 4 of Op[clean_email_mapper] based on the required memory: NoneGB and required cpu: 1.\u001b[0m\n",
                        "\u001b[32m2026-02-12 09:28:07.923\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mdata_juicer.utils.resource_utils\u001b[0m:\u001b[36mquery_cuda_info\u001b[0m:\u001b[36m44\u001b[0m - \u001b[33m\u001b[1mCommand nvidia-smi is not found. There might be no GPUs on this machine.\u001b[0m\n",
                        "clean_email_mapper_process (num_proc=4): 100%|██████████| 7/7 [00:00<00:00, 26.87 examples/s]\n",
                        "\u001b[32m2026-02-12 09:28:08.463\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.core.data.dj_dataset\u001b[0m:\u001b[36mprocess\u001b[0m:\u001b[36m310\u001b[0m - \u001b[1m[2/5] OP [clean_email_mapper] Done in 0.581s. Left 7 samples.\u001b[0m\n",
                        "\u001b[32m2026-02-12 09:28:08.501\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.utils.process_utils\u001b[0m:\u001b[36mcalculate_np\u001b[0m:\u001b[36m161\u001b[0m - \u001b[1mSet the auto `num_proc` to 4 of Op[language_id_score_filter] based on the required memory: NoneGB and required cpu: 1.\u001b[0m\n",
                        "\u001b[32m2026-02-12 09:28:08.505\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mdata_juicer.utils.resource_utils\u001b[0m:\u001b[36mquery_cuda_info\u001b[0m:\u001b[36m44\u001b[0m - \u001b[33m\u001b[1mCommand nvidia-smi is not found. There might be no GPUs on this machine.\u001b[0m\n",
                        "Adding new column for stats (num_proc=4): 100%|██████████| 7/7 [00:00<00:00, 31.49 examples/s]\n",
                        "\u001b[32m2026-02-12 09:28:08.739\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.utils.process_utils\u001b[0m:\u001b[36mcalculate_np\u001b[0m:\u001b[36m161\u001b[0m - \u001b[1mSet the auto `num_proc` to 4 of Op[language_id_score_filter] based on the required memory: NoneGB and required cpu: 1.\u001b[0m\n",
                        "language_id_score_filter_compute_stats (num_proc=4):   0%|          | 0/7 [00:00<?, ? examples/s]\u001b[32m2026-02-12 09:28:08.872\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.utils.model_utils\u001b[0m:\u001b[36mprepare_fasttext_model\u001b[0m:\u001b[36m502\u001b[0m - \u001b[1mLoading fasttext language identification model...\u001b[0m\n",
                        "\u001b[32m2026-02-12 09:28:08.878\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.utils.model_utils\u001b[0m:\u001b[36mprepare_fasttext_model\u001b[0m:\u001b[36m502\u001b[0m - \u001b[1mLoading fasttext language identification model...\u001b[0m\n",
                        "\u001b[32m2026-02-12 09:28:08.887\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.utils.model_utils\u001b[0m:\u001b[36mprepare_fasttext_model\u001b[0m:\u001b[36m502\u001b[0m - \u001b[1mLoading fasttext language identification model...\u001b[0m\n",
                        "\u001b[32m2026-02-12 09:28:08.893\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.utils.model_utils\u001b[0m:\u001b[36mprepare_fasttext_model\u001b[0m:\u001b[36m502\u001b[0m - \u001b[1mLoading fasttext language identification model...\u001b[0m\n",
                        "language_id_score_filter_compute_stats (num_proc=4): 100%|██████████| 7/7 [00:00<00:00, 11.30 examples/s]\n",
                        "\u001b[32m2026-02-12 09:28:09.374\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.utils.process_utils\u001b[0m:\u001b[36mcalculate_np\u001b[0m:\u001b[36m161\u001b[0m - \u001b[1mSet the auto `num_proc` to 4 of Op[language_id_score_filter] based on the required memory: NoneGB and required cpu: 1.\u001b[0m\n",
                        "language_id_score_filter_process (num_proc=4): 100%|██████████| 7/7 [00:00<00:00, 28.54 examples/s]\n",
                        "\u001b[32m2026-02-12 09:28:10.062\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.core.data.dj_dataset\u001b[0m:\u001b[36mprocess\u001b[0m:\u001b[36m310\u001b[0m - \u001b[1m[3/5] OP [language_id_score_filter] Done in 1.598s. Left 5 samples.\u001b[0m\n",
                        "\u001b[32m2026-02-12 09:28:10.095\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.utils.process_utils\u001b[0m:\u001b[36mcalculate_np\u001b[0m:\u001b[36m161\u001b[0m - \u001b[1mSet the auto `num_proc` to 4 of Op[text_length_filter] based on the required memory: NoneGB and required cpu: 1.\u001b[0m\n",
                        "\u001b[32m2026-02-12 09:28:10.098\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mdata_juicer.utils.resource_utils\u001b[0m:\u001b[36mquery_cuda_info\u001b[0m:\u001b[36m44\u001b[0m - \u001b[33m\u001b[1mCommand nvidia-smi is not found. There might be no GPUs on this machine.\u001b[0m\n",
                        "text_length_filter_compute_stats (num_proc=4): 100%|██████████| 5/5 [00:00<00:00, 17.42 examples/s]\n",
                        "\u001b[32m2026-02-12 09:28:10.420\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.utils.process_utils\u001b[0m:\u001b[36mcalculate_np\u001b[0m:\u001b[36m161\u001b[0m - \u001b[1mSet the auto `num_proc` to 4 of Op[text_length_filter] based on the required memory: NoneGB and required cpu: 1.\u001b[0m\n",
                        "text_length_filter_process (num_proc=4): 100%|██████████| 5/5 [00:00<00:00, 18.10 examples/s]\n",
                        "\u001b[32m2026-02-12 09:28:11.143\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.core.data.dj_dataset\u001b[0m:\u001b[36mprocess\u001b[0m:\u001b[36m310\u001b[0m - \u001b[1m[4/5] OP [text_length_filter] Done in 1.080s. Left 4 samples.\u001b[0m\n",
                        "\u001b[32m2026-02-12 09:28:11.179\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.utils.process_utils\u001b[0m:\u001b[36mcalculate_np\u001b[0m:\u001b[36m161\u001b[0m - \u001b[1mSet the auto `num_proc` to 4 of Op[alphanumeric_filter] based on the required memory: NoneGB and required cpu: 1.\u001b[0m\n",
                        "\u001b[32m2026-02-12 09:28:11.183\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mdata_juicer.utils.resource_utils\u001b[0m:\u001b[36mquery_cuda_info\u001b[0m:\u001b[36m44\u001b[0m - \u001b[33m\u001b[1mCommand nvidia-smi is not found. There might be no GPUs on this machine.\u001b[0m\n",
                        "alphanumeric_filter_compute_stats (num_proc=4): 100%|██████████| 4/4 [00:00<00:00, 12.81 examples/s]\n",
                        "\u001b[32m2026-02-12 09:28:11.532\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.utils.process_utils\u001b[0m:\u001b[36mcalculate_np\u001b[0m:\u001b[36m161\u001b[0m - \u001b[1mSet the auto `num_proc` to 4 of Op[alphanumeric_filter] based on the required memory: NoneGB and required cpu: 1.\u001b[0m\n",
                        "alphanumeric_filter_process (num_proc=4): 100%|██████████| 4/4 [00:00<00:00, 14.77 examples/s]\n",
                        "\u001b[32m2026-02-12 09:28:12.227\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.core.data.dj_dataset\u001b[0m:\u001b[36mprocess\u001b[0m:\u001b[36m310\u001b[0m - \u001b[1m[5/5] OP [alphanumeric_filter] Done in 1.084s. Left 4 samples.\u001b[0m\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Processed dataset: 4 samples\n",
                        "\n",
                        "Final output:\n",
                        "1. This is a high-quality English text sample.\n",
                        "2. Contact us at  for more information.\n",
                        "3. Machine learning is transforming industries worldwide.\n",
                        "4. This has extra whitespace issues.\n"
                    ]
                }
            ],
            "source": [
                "# Method 2: Using process() with operator list\n",
                "dataset = Dataset.from_list(samples)\n",
                "\n",
                "# Define operator pipeline\n",
                "operators = [\n",
                "    WhitespaceNormalizationMapper(),\n",
                "    CleanEmailMapper(),\n",
                "    LanguageIDScoreFilter(lang='en', min_score=0.8),\n",
                "    TextLengthFilter(min_len=20, max_len=200),\n",
                "    AlphanumericFilter(min_ratio=0.5)\n",
                "]\n",
                "\n",
                "# Apply all operators in one call\n",
                "dataset = dataset.process(operators)\n",
                "\n",
                "print(f\"Processed dataset: {len(dataset)} samples\")\n",
                "print(\"\\nFinal output:\")\n",
                "for i, sample in enumerate(dataset, 1):\n",
                "    print(f\"{i}. {sample['text']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Inspect Operator Statistics\n",
                "\n",
                "Filter operators can be configured to compute statistics without filtering. This helps you understand your dataset characteristics before deciding on filter thresholds."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[32m2026-02-12 09:28:16.496\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.utils.process_utils\u001b[0m:\u001b[36mcalculate_np\u001b[0m:\u001b[36m161\u001b[0m - \u001b[1mSet the auto `num_proc` to 4 of Op[text_length_filter] based on the required memory: NoneGB and required cpu: 1.\u001b[0m\n",
                        "Adding new column for stats (num_proc=4): 100%|██████████| 7/7 [00:00<00:00, 28.57 examples/s]\n",
                        "\u001b[32m2026-02-12 09:28:16.753\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata_juicer.utils.process_utils\u001b[0m:\u001b[36mcalculate_np\u001b[0m:\u001b[36m161\u001b[0m - \u001b[1mSet the auto `num_proc` to 4 of Op[text_length_filter] based on the required memory: NoneGB and required cpu: 1.\u001b[0m\n",
                        "text_length_filter_compute_stats (num_proc=4): 100%|██████████| 7/7 [00:00<00:00, 26.14 examples/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Text length statistics:\n",
                        "1. Text: This is a high-quality English text sample....\n",
                        "   Length: 43 chars\n",
                        "2. Text: Short...\n",
                        "   Length: 5 chars\n",
                        "3. Text: Contact us at support@example.com for more informa...\n",
                        "   Length: 55 chars\n",
                        "4. Text: Bonjour! Ceci est un texte en français....\n",
                        "   Length: 39 chars\n",
                        "5. Text: Machine learning is transforming industries worldw...\n",
                        "   Length: 54 chars\n",
                        "6. Text: a@#$%^&*()_+{}[]|\\:;<>?,./...\n",
                        "   Length: 26 chars\n",
                        "7. Text: This has\textra\twhitespace　issues....\n",
                        "   Length: 33 chars\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Create fresh dataset\n",
                "dataset = Dataset.from_list(samples)\n",
                "\n",
                "# Compute statistics without filtering\n",
                "length_filter = TextLengthFilter(min_len=20, max_len=200)\n",
                "dataset_with_stats = length_filter.run(dataset, reduce=False)  # Compute stats without filtering\n",
                "\n",
                "# Check statistics\n",
                "print(\"Text length statistics:\")\n",
                "for i, sample in enumerate(dataset_with_stats, 1):\n",
                "    stats = sample.get('__dj__stats__', {})\n",
                "    print(f\"{i}. Text: {sample['text'][:50]}...\")\n",
                "    print(f\"   Length: {stats.get('text_len', 'N/A')} chars\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Further Reading\n",
                "\n",
                "- [Complete Operators List](https://datajuicer.github.io/data-juicer/en/main/docs/Operators.html)\n",
                "- [Building Recipes with YAML](./02_Building_Recipes.ipynb)\n",
                "- [Developer Guide](https://datajuicer.github.io/data-juicer/en/main/docs/DeveloperGuide.html)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "data-juicer-hub (3.11.14)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
